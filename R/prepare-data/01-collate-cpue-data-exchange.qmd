---
title: "Collate CPUE data"
author: "Max Lindmark"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: 
  html:
    embed-resources: true
    fig-width: 8
    #fig-asp: 0.618
knitr: 
  opts_chunk:
    fig.align: center
editor: source
execute: 
  echo: true
  eval: true
  cache: false
---

# Intro
In this script, I load exchange data from datras and calculate catch of cod and flounder in unit kg/km^2 (with TVL gear) by size group, by correcting for gear dimensions, sweeplength and trawl speed, following Orio et al 2017. 

## Load libraries

```{r set up}
#| warning: false
#| message: false

# Load libraries, install if needed
library(tidyverse)
library(readxl)
library(RCurl)
library(devtools)
library(patchwork)
library(janitor)
library(viridis)
library(icesDatras)
library(mapdata)
library(raster)
library(ncdf4)
library(marmap)
library(modelr)
library(tidync)
library(tidyterra)
library(tidylog)
library(sdmTMB)
# devtools::install_github("seananderson/ggsidekick") # not on CRAN 
library(ggsidekick)
theme_set(theme_sleek())

# Source code for map plots
source_url("https://raw.githubusercontent.com/maxlindmark/cod-interactions/main/R/functions/map-plot.R")
 
# Set path
home <- here::here()
```

## Read data

```{r read data}
#| message: false

# Data were read in from getDATRAS on 2024.06.10
# Read HH data
#bits_hh <- getDATRAS(record = "HH", survey = "BITS", years = 1991:2023, quarters = c(1, 4))
#write.csv(bits_hh, "data/DATRAS/bits_hh.csv")
bits_hh <- read_csv(paste0(home, "/data/DATRAS/bits_hh.csv")) |> filter(Year > 1992) # To match covariates

# Read HL data
#bits_hl <- getDATRAS(record = "HL", survey = "BITS", years = 1991:2023, quarters = c(1, 4))
#write.csv(bits_hl, "data/DATRAS/bits_hl.csv")
bits_hl <- read_csv(paste0(home, "/data/DATRAS/bits_hl.csv")) |> filter(Year > 1992) # To match covariates

# Read CA data
#bits_ca <- getDATRAS(record = "CA", survey = "BITS", years = 1991:2023, quarters = c(1, 4))
#write.csv(bits_ca, "data/DATRAS/bits_ca.csv")
bits_ca <- read_csv(paste0(home, "/data/DATRAS/bits_ca.csv")) |> filter(Year > 1992) # To match covariates

# Read gear standardization data 
sweep <- read.csv(paste0(home, "/data/cpue-standardization/sweep_9116.csv"), sep = ";", dec = ",", fileEncoding = "latin1")
sweep <- read.csv(paste0(home, "/data/cpue-standardization/sweep_9118_ml.csv"), sep = ";", fileEncoding = "latin1")
```

## Standardize catch data
#### Standardize ships
```{r ships}
#| message: false

# Before creating a new ID, make sure that countries and ships names use the same format
sort(unique(sweep$Ship))
sort(unique(bits_hh$Ship))
sort(unique(bits_hl$Ship))

# Change back to the old Ship name standard...
# https://vocab.ices.dk/?ref=315
# https://vocab.ices.dk/?ref=315
# Assumptions:
# SOL is Solea on ICES links above, and SOL1 is the older one of the two SOLs (1 and 2)
# DAN is Dana
# sweep |> filter(Ship == "DANS") |> distinct(Year, Country)
# sweep |> filter(Ship == "DAN2") |> distinct(Year)
# bits_hh |> filter(Ship == "67BC") |> distinct(Year, Country)
# sweep |> filter(Ship == "DAN2") |> distinct(Year)
# bits_hh |> filter(Ship == "26D4") |> distinct(Year) # Strange that 26DF doesn't extend far back. Which ship did the Danes use? Ok, I have no Danish data that old.
# bits_hh |> filter(Country == "DK") |> distinct(Year)

bits_hh <- bits_hh |>
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) |> 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

bits_hl <- bits_hl |>
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) |> 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

bits_ca <- bits_ca |>
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) |> 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

# Ok, which ships are missing in the exchange data?
unique(bits_hh$Ship3)[!unique(bits_hh$Ship3) %in% unique(sweep$Ship)]
# Swedish Ships and unidentified ships are NOT in the Sweep data
unique(sweep$Ship3)[!unique(sweep$Ship3) %in% unique(bits_hh$Ship3)]
# But all Sweep Ships are in the exchange data
```

#### Standardize countries

```{r countries}
#| message: false

# Now check which country codes are used
sort(unique(sweep$Country))
sort(unique(bits_hh$Country))

# https://www.nationsonline.org/oneworld/country_code_list.htm#E
bits_hh <- bits_hh |>
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

bits_hl <- bits_hl |>
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

bits_ca <- bits_ca |>
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

# Gear? Are they the same?
sort(unique(bits_hh$Gear))
sort(unique(bits_hl$Gear))
sort(unique(sweep$Gear))

# Which gears are NOT in the sweep data?
unique(bits_hl$Gear)[!unique(bits_hl$Gear) %in% unique(sweep$Gear)] 
```

#### Create a simple haul ID that works across all exchange data

```{r haul id}
#| message: false

# Create ID column
bits_ca <- bits_ca |> 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_hl <- bits_hl |> 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_hh <- bits_hh |> 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

# Works like a haul-id
bits_hh |> group_by(IDx) |> mutate(n = n()) |> ungroup() |> distinct(n)
```

#### Create the same unique haul-ID in the cpue data that I have in the sweep-file

```{r haul id check}
#| message: false

bits_hl <- bits_hl |> 
  mutate(haul.id = paste(Year, Quarter, Country, Ship3, Gear, StNo, HaulNo, sep = ":")) 

bits_hh <- bits_hh |> 
  mutate(haul.id = paste(Year, Quarter, Country, Ship3, Gear, StNo, HaulNo, sep = ":")) 

bits_hh |> group_by(haul.id) |> mutate(n = n()) |> ungroup() |> distinct(n)
```

#### Clean DATRAS EXCHANGE data

```{r filter hauls}
#| message: false

# Select just valid, additional and no oxygen hauls
bits_hh <- bits_hh |>
  #filter(!Country == "SWE") |> # I'll deal with Sweden later...
  filter(HaulVal %in% c("A","N","V"))

# Add ICES rectangle
bits_hh$Rect <- mapplots::ices.rect2(lon = bits_hh$ShootLong, lat = bits_hh$ShootLat)

# Add ICES subdivisions
shape <- shapefile(paste0(home, "/data/shapefiles/ICES-StatRec-mapto-ICES-Areas/StatRec_map_Areas_Full_20170124.shp"))

pts <- SpatialPoints(cbind(bits_hh$ShootLong, bits_hh$ShootLat), 
                     proj4string = CRS(proj4string(shape)))

bits_hh$sub_div <- over(pts, shape)$Area_27

# Rename subdivisions to the more common names and do some more filtering (by sub div and area)
sort(unique(bits_hh$sub_div))

bits_hh <- bits_hh |> 
  mutate(sub_div = factor(sub_div),
         sub_div = fct_recode(sub_div,
                              "20" = "3.a.20",
                              "21" = "3.a.21",
                              "22" = "3.c.22",
                              "23" = "3.b.23",
                              "24" = "3.d.24",
                              "25" = "3.d.25",
                              "26" = "3.d.26",
                              "27" = "3.d.27",
                              "28" = "3.d.28.1",
                              "28" = "3.d.28.2",
                              "29" = "3.d.29"),
         sub_div = as.character(sub_div)) 

# Now add the fishing line information from the sweep file (we need that later
# to standardize based on gear geometry). We add in the the HH data and then
# transfer it to the other exchange data files when left_joining.
# Check which Fishing lines I have in the sweep data:
fishing_line <- sweep |> group_by(Gear) |> distinct(Fishing.line)

bits_hh <- left_join(bits_hh, fishing_line)
# sweep |> group_by(Gear) |> distinct(Fishing.line)
# bits_hh |> group_by(Gear) |> distinct(Fishing.line)
bits_hh$Fishing.line <- as.numeric(bits_hh$Fishing.line)

# Which gears do now have fishing line?
bits_hh$Fishing.line[is.na(bits_hh$Fishing.line)] <- -9
bits_hh |> filter(Fishing.line == -9) |> distinct(Gear)
# 1  GRT
# 2  CAM
# 3  EXP
# 4  FOT
# 5  GOV
# 6  EGY
# 7   DT
# 8  ESB
# 9  HAK

# FROM the index files (Orio, "Research Östersjön 2")
# FOT has 83
# GOV has 160
# ESB ??
# GRT ??
# Rest are unknown and likely not used by Swedish data (therefore their correction
# factors my be in the sweep file)

# Add these values:
bits_hh <- bits_hh |> mutate(Fishing.line = ifelse(Gear == "FOT", 83, Fishing.line))
bits_hh <- bits_hh |> mutate(Fishing.line = ifelse(Gear == "GOV", 160, Fishing.line))

# Now select the hauls in the HH data when subsetting the HL data
bits_hl <- bits_hl |>
  filter(haul.id %in% bits_hh$haul.id)

# Match columns from the HH data to the HL and CA data
sort(unique(bits_hh$sub_div))
sort(colnames(bits_hh))

# No NAs for the variables going in to the stomach haul ID
unique(is.na(bits_hh |> dplyr::select(Year, Quarter, Month, Country, Rect, HaulNo)))

# Before making the id_haul_stomach variable we need to change the country column so that it actually matches the stomach data
# This is the stomach version:
#[1] "LV" "PL" "SE" "DK"
unique(bits_hh$Country)

# MAKE SURE THE COUNTRY CODE IS THE SAME! FOR NOW I DON*T USE COUNTRY 2
bits_hh <- bits_hh |> mutate(Country2 = NA,
                              Country2 = ifelse(Country == "LAT", "LV", Country2),
                              Country2 = ifelse(Country == "POL", "PL", Country2),
                              Country2 = ifelse(Country == "SWE", "SE", Country2),
                              Country2 = ifelse(Country == "DEN", "DK", Country2))

bits_hh_merge <- bits_hh |> 
  mutate(id_haul_stomach = paste(Year, Quarter, Month, Country, Rect, HaulNo, sep = ".")) |> 
  dplyr::select(sub_div, Rect, HaulVal, StdSpecRecCode, BySpecRecCode, Fishing.line, Month,
                DataType, HaulDur, GroundSpeed, haul.id, IDx, ShootLat, ShootLong, id_haul_stomach)

bits_hl <- left_join(dplyr::select(bits_hl, -haul.id), bits_hh_merge, by = "IDx")
bits_ca <- left_join(bits_ca, bits_hh_merge, by = "IDx")

# Now filter the subdivisions I want from all data sets
bits_hh <- bits_hh |> filter(sub_div %in% c(24, 25, 26, 27, 28))
bits_hl <- bits_hl |> filter(sub_div %in% c(24, 25, 26, 27, 28))
bits_ca <- bits_ca |> filter(sub_div %in% c(24, 25, 26, 27, 28))
```

```{r test}
#| include: false
#| eval: false
bits_hl |> filter(Year == 2016 & Quarter == 1 & Month == 2 & Country == "SWE" & Rect == "39G4") |> distinct(HaulNo)
```

#### Filter species

```{r filter species}
#| message: false

hlcod <- bits_hl |>
  filter(SpecCode %in% c("126436", "164712")) |> 
  mutate(Species = "Gadus morhua")
```

#### Prepare to add 0 catches

```{r add zero catches}
#| message: false

# Find common columns in the HH and HL data (here already subset by species)
comcol <- intersect(names(hlcod), names(bits_hh))

# What is the proportion of zero-catch hauls?
# Here we don't have zero catches
hlcod |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(HLNoAtLngt)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |> 
  distinct(zero_catch)

# Cod: Add 0s and then remove lines with SpecVal = 0 (first NA because we don't have a match in the HH, then make them 0 later)
hlcod0 <- full_join(hlcod, bits_hh[, comcol], by = comcol)

# No zeroes yet
hlcod0 |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(HLNoAtLngt)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |> 
  distinct(zero_catch) 

hlcod0$SpecVal[is.na(hlcod0$SpecVal)] <- "zeroCatch"

hlcod0$SpecVal <- factor(hlcod0$SpecVal)

hlcod0 <-  hlcod0 |> filter(!SpecVal == "0")

# Add species again after merge
hlcod0$Species <- "Gadus morhua"
```

#### Create (unstandardized) CPUE for `SpecVal=1`. If `DataType=C` then `CPUEun=HLNoAtLngt`, if `DataType=R` then `CPUEun=HLNoAtLngt/(HaulDur/60)`, if `DataType=S` then `CPUEun=(HLNoAtLngt*SubFactor)/(HaulDur/60)`. If `SpecVal="zeroCatch"` then `CPUEun=0`, if `SpecVal=4` we need to decide (no length measurements, only total catch). Note that here we also add zero CPUE if `SpecVal=="zeroCatch"`.

Then I will sum for the same haul the CPUE of the same length classes if they were sampled with different subfactors or with different sexes.

```{r calculate cpue}
#| message: false

# Cod
hlcod0 <- hlcod0 |>
  mutate(CPUEun = ifelse(SpecVal == "1" & DataType == "C",
                         HLNoAtLngt,
                         
                         ifelse(SpecVal == "1" & DataType == "R",
                                HLNoAtLngt/(HaulDur/60),
                                
                                ifelse(SpecVal == "1" & DataType == "S",
                                       (HLNoAtLngt*SubFactor)/(HaulDur/60),
                                       
                                       ifelse(SpecVal == "zeroCatch", 0, NA)))))

# Plot and fill by zero catch
hlcod0 |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  filter(!CPUEun_haul == 0)

hlcod0 |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |>
  group_by(Year, zero_catch) |> 
  summarise(n = n()) |> 
  ggplot(aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")

# Some rows have multiple rows per combination of length class and haul id, so we need to sum it up 
hlcod0 |> group_by(LngtClass, haul.id) |> mutate(n = n()) |> ungroup() |> distinct(n)
hlcod0 |> group_by(LngtClass, haul.id) |> mutate(n = n()) |> ungroup() |> filter(n == 2) |> as.data.frame() |> head(20)
test <- hlcod0 |> group_by(LngtClass, haul.id) |> mutate(n = n()) |> ungroup() |> filter(n == 2)
test_id <- test$haul.id[2]

hlcodL <- hlcod0 |> 
  group_by(LngtClass, haul.id) |> 
  mutate(CPUEun = sum(CPUEun)) |>
  ungroup() |> 
  mutate(id3 = paste(haul.id, LngtClass)) |> 
  distinct(id3, .keep_all = TRUE) |> 
  dplyr::select(-id3) # Clean up a bit

# Check with an ID
filter(hlcod0, haul.id == test_id)
filter(hlcodL, haul.id == test_id) |> as.data.frame()

# Do we still have 0 catches?
hlcodL |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  filter(!CPUEun_haul == 0)

hlcodL |>
  group_by(haul.id, Year) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |>
  group_by(Year, zero_catch) |> 
  summarise(n = n()) |> 
  ggplot(aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")
```

#### Get and add annual weight-length relationships from the CA data for both cod and flounder so that I can calculate CPUE in biomass rather than numbers further down

```{r lw pars}
#| message: false

# Cod
bits_ca_cod <- bits_ca |> 
  filter(SpecCode %in% c("164712", "126436")) |> 
  mutate(StNo = as.numeric(StNo)) |> 
  mutate(Species = "Cod") |> 
  mutate(ID = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

# Now I need to copy rows with NoAtLngt > 1 so that 1 row = 1 ind
bits_ca_cod <- bits_ca_cod %>% map_df(., rep, .$CANoAtLngt)

# Standardize length and drop NA weights (need that for condition)
bits_ca_cod <- bits_ca_cod |> 
  drop_na(IndWgt) |> 
  drop_na(LngtClass) |> 
  filter(IndWgt > 0 & LngtClass > 0) |>  # Filter positive length and weight
  mutate(weight_kg = IndWgt/1000) |> 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) # Standardize length ((https://vocab.ices.dk/?ref=18))

# Plot
ggplot(bits_ca_cod, aes(IndWgt, length_cm)) +
  geom_point() + 
  facet_wrap(~Year)

# Now extract the coefficients for each year (not bothering with outliers at the moment)
cod_intercept <- bits_ca_cod |> 
  split(~Year) |> 
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) |>
  purrr::map_df(broom::tidy, .id = 'Year') |>
  filter(term == "(Intercept)") |> 
  mutate(a = exp(estimate)) |> 
  mutate(Year = as.integer(Year)) |> 
  dplyr::select(Year, a)

cod_slope <- bits_ca_cod |>
  split(~Year) |>
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) |>
  purrr::map_df(broom::tidy, .id = 'Year') |>
  filter(term == "log(length_cm)") |> 
  mutate(Year = as.integer(Year)) |> 
  rename("b" = "estimate") |> 
  dplyr::select(Year, b)
```

#### Join the annual L-W relationships to the respective catch data to calculate CPUE in biomass not abundance

```{r join lw}
#| message: false

hlcodL <- left_join(hlcodL, cod_intercept, by = "Year")
hlcodL <- left_join(hlcodL, cod_slope, by = "Year")
```

#### Convert from CPUE in numbers to kg

```{r cpue in weight}
#| message: false

# First standardize length to cm and then check how zero-catches are implemented at this stage
hlcodL <- hlcodL |> 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) # Standardize length ((https://vocab.ices.dk/?ref=18))

filter(hlcodL, length_cm == 0) # No such thing

# Now check if all rows where length is NA are the ones with zero catch!
hlcodL |> 
  mutate(length2 = replace_na(length_cm, -9),
         no_length = ifelse(length2 < 0, "T", "F")) |> 
  ggplot(aes(length2, CPUEun, color = no_length)) + geom_point(alpha = 0.2) +
  facet_wrap(~no_length)

hlcodL |> filter(CPUEun == 0) |> distinct(length_cm)

# Right, so all hauls with zero catch have NA length_cm. I don't have any NA catches
t <- hlcodL |> drop_na(CPUEun)
t <- hlcodL |> filter(CPUEun == 0)
t <- hlcodL |> drop_na(length_cm)

# In other words, a zero catch is when the catch is zero and length_cm is NA
# In order to not get any NA CPUEs in unit biomass because length is NA (I want them instead
# to be 0, as the numbers-CPUE is), I will replace length_cm == NA with length_cm == 0 before
# calculating biomass CPUE
hlcodL <- hlcodL |> mutate(length_cm2 = replace_na(length_cm, 0))

# Standardize length in the haul-data and calculate weight
hlcodL <- hlcodL |> 
  mutate(weight_kg = (a*length_cm2^b)/1000) |> 
  mutate(CPUEun_kg = weight_kg*CPUEun)

# Plot and check it's correct also in this data
ggplot(hlcodL, aes(weight_kg, length_cm2)) +
  geom_point() + 
  facet_wrap(~Year)

# Hmm, some unrealistic weights actually
hlcodL |> arrange(desc(weight_kg)) |> as.data.frame() |> head(20)
hlcodL <- hlcodL |> filter(weight_kg < 100 & length_cm2 < 135)

ggplot(hlcodL, aes(weight_kg, length_cm2)) +
  geom_point() + 
  facet_wrap(~Year)

# What is the proportion of zero-catch hauls?
hlcodL |>
  group_by(haul.id) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  filter(CPUEun_haul == 0)
  
cod_0plot <- hlcodL |>
  group_by(haul.id, Year, Quarter) |>
  summarise(CPUEun_haul = sum(CPUEun)) |> 
  ungroup() |> 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) |> 
  group_by(Year, Quarter, zero_catch) |> 
  summarise(n = n()) |> 
  ungroup() |> 
  pivot_wider(names_from = zero_catch, values_from = n) |> 
  mutate(prop_zero_catch_hauls = Y/(N+Y)) |> 
  ggplot(aes(Year, prop_zero_catch_hauls)) + geom_bar(stat = "identity") + 
  coord_cartesian(expand = 0, ylim = c(0, 1)) + 
  facet_wrap(~ Quarter) +
  ggtitle("Cod")
```

#### Standardize according to Orio
To get unit: kg of fish caught by trawling for 1 h a standard bottom swept area of 0.45km2 using a TVL trawl with 75 m sweeps at the standard speed of three knots

```{r standardize}
#| message: false

# Remove hauls done with the TVL gear with a SweepLngt < 50 (these are calibration hauls, pers. com. Anders & Ale)
# And also hauls without length-information
# Remove pelagic gear
hlcodL <- hlcodL |>
  mutate(SweepLngt2 = replace_na(SweepLngt, 50)) |> 
  mutate(keep = ifelse(Gear == "TVL" & SweepLngt2 < 50, "N", "Y")) |> 
  filter(keep == "Y") |> 
  dplyr::select(-keep, -SweepLngt2) |> 
  filter(!Gear == "PEL")
  
# Add in RS and RSA-values from the sweep file
# CPUE should be multiplied with RS and RSA to standardize to a relative speed and gear dimension.
# There is not a single file will all RS and RSA values. Instead they come in three files:
# - sweep (non-Swedish hauls between 1991-2016)
# - + calculated based on trawl speed and gear dimensions.
# I will join in the RS and RSA values from all sources, then standardize and filter
# away non-standardized hauls
# sort(unique(sweep$Year))
# sort(unique(sweep$Country))

# Since I don't have the sweep data for Swedish data, I have to calculate it from scratch using the 
# equation in Orio's spreadsheet

# First I will join in the sweep data, 
sweep_sel <- sweep |> rename("haul.id" = "ï..haul.id") |> dplyr::select(haul.id, RSA, RS)

hlcodL2 <- left_join(hlcodL, sweep_sel)

hlcodL2 <- hlcodL2 |>
  rename("RS_sweep" = "RS",
         "RSA_sweep" = "RSA") |> 
  mutate(RS_sweep = as.numeric(RS_sweep),
         RSA_sweep = as.numeric(RSA_sweep))

sort(colnames(hlcodL2))

# I will calculate a RS and RSA column in the catch data based on Ale's equation in the sweep file:
sort(unique(hlcodL2$GroundSpeed))
sort(unique(hlcodL2$Fishing.line))
sort(unique(hlcodL2$SweepLngt))

# First replace -9 in the columns I use for the calculations with NA so I don't end up with real numbers that are wrong!
hlcodL2 <- hlcodL2 |> mutate(GroundSpeed = ifelse(GroundSpeed == -9, NA, GroundSpeed),
                              Fishing.line = ifelse(Fishing.line == -9, NA, Fishing.line),
                              SweepLngt = ifelse(SweepLngt == -9, NA, SweepLngt))

hlcodL2 |> filter(Quarter == 1) |>
  distinct(GroundSpeed, Fishing.line, SweepLngt) |> as.data.frame() |> head(20)

hlcodL2 |> filter(Quarter == 4) |>
  distinct(GroundSpeed, Fishing.line, SweepLngt) |> as.data.frame() |> head(20)

# Hmm, Q1 has at least one of the RS or RSA variables as NAs. Will be difficult to standardize!
# Hope the correction factors are present in Ales conversion data

# Now calculate correction factors
hlcodL2 <- hlcodL2 |> mutate(RS_x = 3/GroundSpeed,
                              Horizontal.opening..m. = Fishing.line*0.67,
                              Swep.one.side..after.formula...meter = 0.258819045*SweepLngt, # SIN(RADIANS(15))
                              Size.final..m = Horizontal.opening..m. + (Swep.one.side..after.formula...meter*2),
                              Swept.area = (Size.final..m*3*1860)/1000000,
                              RSA_x = 0.45388309675081/Swept.area)

# Check EQ. is correct by recalculating it in the sweep file
sweep <- sweep |> mutate(Horizontal.opening..m.2 = Fishing.line*0.67,
                          Swep.one.side..after.formula...meter2 = 0.258819045*SweepLngt, # SIN(RADIANS(15))
                          Size.final..m2 = Horizontal.opening..m.2 + (Swep.one.side..after.formula...meter2*2),
                          Swept.area2 = (Size.final..m2*3*1860)/1000000,
                          RSA_x = 0.45388309675081/Swept.area2)

sweep |>
  drop_na() |>
  ggplot(aes(as.numeric(RSA), RSA_x)) + geom_point() + geom_abline(intercept = 0, slope = 1)
# Yes it's the same

# Replace NAs with -1/3 (because ICES codes missing values as -9 and in the calculation above they get -1/3),
# so that I can filter them easily later
# sort(unique(hlcodL2$RS_x))
# sort(unique(hlcodL2$RSA_x))

hlcodL2$RS_x[is.na(hlcodL2$RS_x)] <- -1/3
hlcodL2$RS_sweep[is.na(hlcodL2$RS_sweep)] <- -1/3
hlcodL2$RSA_x[is.na(hlcodL2$RSA_x)] <- -1/3
hlcodL2$RSA_sweep[is.na(hlcodL2$RSA_sweep)] <- -1/3

# Compare the difference correction factors (calculated vs imported from sweep file)
p1 <- ggplot(filter(hlcodL2, RS_x > 0), aes(RS_x)) + geom_histogram() + xlim(0.4, 1.76)
p2 <- ggplot(hlcodL2, aes(RSA_x)) + geom_histogram()
p3 <- ggplot(hlcodL2, aes(RS_sweep)) + geom_histogram()
p4 <- ggplot(hlcodL2, aes(RSA_sweep)) + geom_histogram()

(p1 + p2) / (p3 + p4)

# Why do I have RSA values smaller than one? (either because sweep length is longer or gear is larger (GOV))
# Check if I can calculate the same RSA in sweep as that entered there.
# Ok, so the equation is correct. Which ID's have RSA < 1?
hlcodL2 |> 
  filter(RSA_x < 1 & RSA_x > 0) |>
  dplyr::select(Year, Country, Ship, Gear, haul.id, Horizontal.opening..m., Fishing.line,
                Swep.one.side..after.formula...meter, SweepLngt, Size.final..m, Swept.area, RSA_x) |> 
  ggplot(aes(RSA_x, fill = factor(SweepLngt))) + geom_histogram() + facet_wrap(~Gear, ncol = 1)

# Check if I have more than one unique RS or RSA value per haul, or if it's "either this or that"
# Filter positive in both columns
hlcodL2 |> filter(RS_x > 0 & RS_sweep > 0) |> 
  ggplot(aes(RS_x, RS_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

hlcodL2 |> filter(RSA_x > 0 & RSA_sweep > 0) |>
  ggplot(aes(RSA_x, RSA_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

# Ok, there's an odd RS_x that is larger than 3. It didn't catch anything and speed is 0.8! Will remove
hlcodL2 <- hlcodL2 |> filter(RS_x < 3)

# Plot again
hlcodL2 |> filter(RS_x > 0 & RS_sweep > 0) |>
  ggplot(aes(RS_x, RS_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

# They are largely the same when they overlap. When they differ, I will use RS_sweep
# Make a single RS and RSA column

# Cod 
hlcodL3 <- hlcodL2 |>
  mutate(RS = -99,
         RS = ifelse(RS_sweep > 0, RS_sweep, RS),
         RS = ifelse(RS < 0 & RS_x > 0, RS_x, RS)) |> # Note that there are no NA i RS_x. This replaces all non-RS_sweep values -0.3, so I can filter positive later
  mutate(RSA = -99,
         RSA = ifelse(RSA_sweep > 0, RSA_sweep, RSA),
         RSA = ifelse(RSA < 0 & RSA_x > 0, RSA_x, RSA)) |>
  #filter(RS > 0) |>
  #filter(RSA > 0) |>
  mutate(RSRSA = RS*RSA)

# Note, here I deviate from Orio (2017) and Lindmark (2022) and instead of removing
# NA values, I replace them with the mean by country and gear, following Smolinsky XXXX
hlcodL3 <- hlcodL3 |>
  mutate(RS = ifelse(RS < 0, NA, RS),
         RSA = ifelse(RSA < 0, NA, RSA)) |> 
  mutate(RSRSA = RS*RSA)

RSRSAmean <- hlcodL3 |>
  group_by(Country, Gear) |>
  summarise(RSRSAmean = mean(RSRSA, na.rm = T))

hlcodL3 <- hlcodL3 |>
  left_join(RSRSAmean) |>
  mutate(RSRSAimputed = ifelse(is.na(RSRSA), 1, 0),
         RSRSA = ifelse(is.na(RSRSA), RSRSAmean, RSRSA))

# Plot
ggplot(hlcodL3, aes(RSRSA)) + geom_histogram()
# Plot
ggplot(hlcodL3, aes(RSRSA)) + geom_histogram()

# Standardize!
hlcodL3 <- hlcodL3 |>
  mutate(CPUEst_kg = CPUEun_kg*RS*RSA,
         CPUEst = CPUEun*RS*RSA)

unique(is.na(hlcodL3$CPUEst_kg))
unique(is.na(hlcodL3$CPUEst))
min(hlcodL3$CPUEst_kg)
min(hlcodL3$CPUEst)

# Now calculate CPUE PER LENGTH CLASS, then create the new unit, i.e.:  convert from kg of fish caught by trawling for 1 h a standard bottom swept area of 0.45km2 (using a TVL trawl with 75 m sweeps at the standard speed of three knots) to... kg of fish per km^2 by dividing with 0.45

p1 <- ggplot(hlcodL3) +
  geom_histogram(aes(length_cm2, fill = "length_cm1"), alpha = 0.5)  

p2 <- ggplot(hlcodL3) +
  geom_histogram(aes(length_cm2, fill = "length_cm2"), alpha = 0.5) 

p1/p2

hlcodhaul <- hlcodL3 |>
  mutate(cpue_kg = CPUEst_kg,
         cpue = CPUEst,
         cpue_kg_un = CPUEun_kg,
         cpue_un = CPUEun,
         density = cpue_kg/0.45,
         density_ab = cpue/0.45)

# t <- hlcodhaul |> filter(haul_cpue_un == 0)
# t <- hlcodhaul |> filter(!Country == "SWE") |> filter(haul_cpue_un > 0)

# First, figure out why i have length = 0 and density = 0 when I have other lengths in the haul
hlcodhaul |> filter(haul.id == "1993:1:GFR:SOL:H20:23:31") |> as.data.frame()

hlcodhaul |>
  group_by(haul.id) |> 
  mutate(no_catches = length(unique(CPUEun))) |> 
  filter(any(CPUEun == 0)) |> 
  filter(no_catches > 1) |> 
  as.data.frame() |> 
  head(20)

hlcodhaul |> 
  group_by(haul.id) |> 
  filter(CPUEun == min(CPUEun)) |> 
  ungroup() |> 
  distinct(CPUEun)

# The minimum CPUE in all hauls is always zero at this stage. It doesn't really matter because I calculate haul-level CPUE by grouping by ID's and summing. But let's remove them anyway

hlcodhaul |>
  group_by(haul.id) |>
  summarise(cpue_haul = sum(cpue)) |> 
  ungroup() |> 
  filter(!cpue_haul == 0)

# Rename columns and select specific columns from the cod data
datcod <- hlcodhaul |>
  dplyr::select(density, Year, ShootLat, ShootLong, Quarter, Country, Month, haul.id, IDx, Rect, sub_div, length_cm2, id_haul_stomach) |> 
  rename(year = Year,
         month = Month,
         lat = ShootLat,
         lon = ShootLong,
         quarter = Quarter,
         ices_rect = Rect,
         length_cm = length_cm2) |> 
  mutate(species = "cod")
```

```{r very long data with all combinations of size and haul id}
#| message: false

# Because it's size-based cpue, I want the data frame to be "full", so that each haul has every size, even if all are empty. Now I only have lengths with catches, and no lengths if catch is zero.
datcod |> group_by(haul.id) |> summarise(n_size = length(unique(length_cm))) |> distinct(n_size, .keep_all = TRUE)
datcod |> filter(haul.id == "1993:1:GFR:SOL:H20:23:31") |> as.data.frame()
datcod |> group_by(haul.id) |> mutate(tot_dens = sum(density)) |> ungroup() |> distinct(haul.id, .keep_all = TRUE) |> filter(tot_dens == 0)

# Create a data frame with all combinations of trawl IDs and lengths
ex_df <- data.frame(expand.grid(
  length_cm = seq_range(datcod$length_cm, by = 1),
  haul.id = unique(datcod$haul.id))
  )

# Create an ID that is haul + length
ex_df$haul.id.size <- paste(ex_df$haul.id, ex_df$length_cm, sep = ".")
datcod$haul.id.size <- paste(datcod$haul.id, datcod$length_cm, sep = ".")

# Remove IDs that are already in datcod
ex_df <- ex_df |> filter(!haul.id.size %in% unique(datcod$haul.id.size)) 

# Add in the other columns besides density and length
dat_for_join <- datcod |> dplyr::select(-density, -length_cm, -haul.id.size) |> distinct(haul.id, .keep_all = TRUE)

ex_df <- left_join(ex_df, dat_for_join, by = "haul.id")

datcod |> filter(haul.id.size %in% ex_df$haul.id.size)

# Bind_rows these data with datcod
nrow(datcod) + nrow(ex_df)

unique(is.na(datcod$density))

datcod <- bind_rows(datcod, ex_df) |> arrange(haul.id, length_cm)
nrow(datcod)
datcod

# Replace NA density with 0 density because that's the added length-classes not previously in the catch data
datcod <- datcod |> mutate(density = replace_na(density, 0))

# Check the proportion zeroes are still correct:
t <- datcod |>
  group_by(haul.id) |>
  summarise(haul_density = sum(density)) |> 
  ungroup()

nrow(datcod)
length(unique(datcod$haul.id))
nrow(t)
t |> filter(!haul_density == 0)

# Rename
dat <- datcod

glimpse(dat)

# Check proportion zeroes
dat |> 
  group_by(haul.id) |> 
  summarise(haul_dens = sum(density)) |> 
  ungroup() |> 
  filter(!haul_dens == 0)

codq4 <- dat |> 
  filter(species == "cod" & quarter == 4) |> 
  group_by(haul.id) |> 
  mutate(haul_dens = sum(density)) |> 
  distinct(haul.id, .keep_all = TRUE) |> 
  mutate(zero_catch = ifelse(haul_dens == 0, "Y", "N")) |> 
  group_by(year, zero_catch) |> 
  summarise(n = n()) |> 
  ungroup() |> 
  pivot_wider(names_from = zero_catch, values_from = n) |> 
  mutate(prop_z = Y / (N+Y), 
         q = 4,
         species = "cod")  

codq1 <- dat |> 
  filter(species == "cod" & quarter == 1) |> 
  group_by(haul.id) |> 
  mutate(haul_dens = sum(density)) |> 
  distinct(haul.id, .keep_all = TRUE) |> 
  mutate(zero_catch = ifelse(haul_dens == 0, "Y", "N")) |> 
  group_by(year, zero_catch) |> 
  summarise(n = n()) |> 
  ungroup() |> 
  pivot_wider(names_from = zero_catch, values_from = n) |> 
  mutate(prop_z = Y / (N+Y), 
         q = 1,
         species = "cod")  

ggplot(bind_rows(codq1, codq4), aes(year, prop_z*100, color = factor(q))) +
  geom_line() +
  facet_wrap(~ species, ncol = 1)
```

## Add in the environmental variables

```{r add env vars}
#| message: false

# Only need 1 row per haul
dat <- dat |>
  mutate(month_year = paste(month, year, sep = "_"))
  
dat_haul <- dat |>
  distinct(haul.id, .keep_all = TRUE) |>
  dplyr::select(lat, lon, year, month, month_year)

covPath <- paste0(home, "/data/covariates")
```

#### Oxygen

```{r oxygen}
#| message: false

# Source: 
# https://data.marine.copernicus.eu/product/BALTICSEA_ANALYSISFORECAST_BGC_003_007/download?dataset=cmems_mod_bal_bgc_anfc_P1M-m_202311
# https://data.marine.copernicus.eu/product/BALTICSEA_MULTIYEAR_BGC_003_012/download?dataset=cmems_mod_bal_bgc_my_P1M-m_202303
# Print details
print(nc_open(paste(covPath, "oxygen", "cmems_mod_bal_bgc_my_P1M-m_1718018848983.nc", sep = "/")))

oxy_tibble <- tidync(paste(covPath, "oxygen",
                           "cmems_mod_bal_bgc_my_P1M-m_1718018848983.nc", sep = "/")) |>
  hyper_tibble() |> 
  mutate(date = as_datetime(time, origin = '1970-01-01')) |>
  mutate(month = month(date),
         day = day(date),
         year = year(date),
         month_year = paste(month, year, sep = "_"),
         oxy = o2b * 0.0223909)

# Now do recent data (forecast)
print(nc_open(paste(covPath, "oxygen", "cmems_mod_bal_bgc_anfc_P1M-m_1718022118172.nc", sep = "/")))

oxy_tibble_new <- tidync(paste(covPath, "oxygen",
                            "cmems_mod_bal_bgc_anfc_P1M-m_1718022118172.nc", sep = "/")) |>
  hyper_tibble() |> 
  mutate(date = as_datetime(time, origin = '1970-01-01')) |>
  mutate(month = month(date),
         day = day(date),
         year = year(date),
         month_year = paste(month, year, sep = "_"),
         oxy = o2b * 0.0223909) |> 
  filter(year > 2021)

oxy_tibble <- bind_rows(oxy_tibble, oxy_tibble_new)

# Loop through all year combos, extract the temperatures at the data locations
oxy_list <- list()

for(i in unique(dat_haul$month_year)) {
  
  d_sub <- filter(dat_haul, month_year == i)
  oxy_tibble_sub <- filter(oxy_tibble, month_year == i)
  
  # Convert to raster
  ggplot(oxy_tibble_sub, aes(longitude, latitude)) + 
    geom_point(size = 0.1)
  
  oxy_raster <- as_spatraster(oxy_tibble_sub, xycols = 2:3,
                              crs = "WGS84", digits = 3)

  ggplot() +
    geom_spatraster(data = oxy_raster$oxy, aes(fill = oxy)) + 
    scale_fill_viridis(option = "magma") +
    ggtitle(i)

  # Extract from raster
  d_sub$oxy <- terra::extract(oxy_raster$oxy,
                              d_sub |> dplyr::select(lon, lat))$oxy  
   
  # Save
  oxy_list[[i]] <- d_sub
  
}

d_oxy <- bind_rows(oxy_list)
```

#### Temperature & Salinity

```{r temperature}
#| message: false

# Source
# https://data.marine.copernicus.eu/product/BALTICSEA_MULTIYEAR_PHY_003_011/description
# https://data.marine.copernicus.eu/product/BALTICSEA_ANALYSISFORECAST_PHY_003_006/description
# Print details
print(nc_open(paste(covPath, "temperature_salinity", "cmems_mod_bal_phy_my_P1M-m_1718087629163.nc", sep = "/")))

st_tibble <- tidync(paste(covPath, "temperature_salinity",
                          "cmems_mod_bal_phy_my_P1M-m_1718087629163.nc", sep = "/")) |>
  hyper_tibble() |> 
  mutate(date = as_datetime(time, origin = '1970-01-01')) |>
  mutate(month = month(date),
         day = day(date),
         year = year(date),
         month_year = paste(month, year, sep = "_"))

# Now do recent data (forecast)
print(nc_open(paste(covPath, "temperature_salinity", "cmems_mod_bal_phy_anfc_P1M-m_1718087127439.nc", sep = "/")))

st_tibble_new <- tidync(paste(covPath, "temperature_salinity",
                              "cmems_mod_bal_phy_anfc_P1M-m_1718087127439.nc", sep = "/")) |>
  hyper_tibble() |> 
  mutate(date = as_datetime(time, origin = '1970-01-01')) |>
  mutate(month = month(date),
         day = day(date),
         year = year(date),
         month_year = paste(month, year, sep = "_")) |> 
  filter(year > 2021)

st_tibble <- bind_rows(st_tibble, st_tibble_new)

# Loop through all year combos, extract the temperatures at the data locations
st_list <- list()

for(i in unique(dat_haul$month_year)) {
  
  d_sub <- filter(dat_haul, month_year == i)
  st_tibble_sub <- filter(st_tibble, month_year == i)
  
  # Convert to raster
  ggplot(st_tibble_sub, aes(longitude, latitude)) + 
    geom_point(size = 0.1)
  
  st_raster <- as_spatraster(st_tibble_sub, xycols = 3:4,
                             crs = "WGS84", digits = 3)

  ggplot() +
    geom_spatraster(data = st_raster$bottomT, aes(fill = bottomT)) + 
    scale_fill_viridis(option = "magma") +
    ggtitle(i)
  
  ggplot() +
    geom_spatraster(data = st_raster$sob, aes(fill = sob)) + 
    scale_fill_viridis(option = "magma") +
    ggtitle(i)

  # Extract from raster
  d_sub$temp <- terra::extract(st_raster$bottomT,
                               d_sub |> dplyr::select(lon, lat))$bottomT
  
  d_sub$salinity <- terra::extract(st_raster$sob,
                                   d_sub |> dplyr::select(lon, lat))$sob  
   
  # Save
  st_list[[i]] <- d_sub
  
}

d_st <- bind_rows(st_list)
```

```{r join environmental data}
#| message: false

d_st2 <- d_st |> 
  mutate(id = paste(month_year, lon, lat)) |> 
  distinct(id, .keep_all = TRUE) |> 
  dplyr::select(-id)

d_oxy2 <- d_oxy |> 
  mutate(id = paste(month_year, lon, lat)) |> 
  distinct(id, .keep_all = TRUE) |> 
  dplyr::select(-id)

env_dat <- d_st2 |> 
  left_join(d_oxy2 |> dplyr::select(-year, -month), by = c("month_year", "lon", "lat")) |> 
  dplyr::select(month_year, lon, lat, temp, salinity, oxy)
```

#### Depth

```{r depth}
#| message: false

# Only use unique locations and then left_join else it will take forever
# https://gis.stackexchange.com/questions/411261/read-multiple-layers-raster-from-ncdf-file-using-terra-package
# https://emodnet.ec.europa.eu/geoviewer/
dep_raster <- terra::rast(paste0(home, "/data/covariates/depth/Mean depth natural colour (with land).nc"))

class(dep_raster)
plot(dep_raster)

env_dat$depth <- terra::extract(dep_raster, env_dat |> dplyr::select(lon, lat))$elevation

ggplot(env_dat, aes(lon, lat, color = depth*-1)) + 
  geom_point() + 
  scale_color_viridis(direction = -1)

env_dat$depth <- env_dat$depth*-1
```

```{r join}
#| message: false

# Now join these data with the full_dat

dat_full <- left_join(dat, env_dat, by = c("month_year", "lon", "lat"))
```

## Add UTM coords

```{r add coords}
#| message: false

# Add UTM coords
dat_full <- dat_full |> add_utm_columns(ll_names = c("lon", "lat"),
                                         utm_crs = 32633)
```

## Save data

```{r save}
#| message: false

dat_full_save <- dat_full |>
  dplyr::select(-IDx, -haul.id.size) |> 
  janitor::clean_names()

write_csv(dat_full_save, file = paste0(home, "/data/clean/catch_by_length.csv"))
```
